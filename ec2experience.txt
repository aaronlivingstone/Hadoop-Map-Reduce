1. How long did each of the six runs take? How many mappers and how many reducers did you use?
Run 1:	20:40 1 mapper and 2 reducers.
Run 2:	6:58 1 mapper and 2 reducers.
Run 3:	16:58 1 mapper and 2 reducers.
Run 4:	10:06 1 mapper and 2 reducers.
Run 5:	9:54 1 mapper and 2 reducers.
Run 6:	9:41 1 mapper and 2 reducers.

2. For the two runs with (freedom, 0), how much faster did your code run on the 5 workers with the combiner turned on than with the combiner turned off? Express your answer as a percentage.
Weirdly ... or not. It ran much faster. Around 197% faster. 

3. For the runs on the 2006 dataset, what was the median processing rate of input for the tests using 5 workers? Using 9 workers? Express your answers in terms of GB/s. (1 GB = 2^30 bytes)
For 5 workers: .0175 GB/s
For 9 workers: .0300 GB/s

4. How much faster was your run of (capital, 0) with 9 workers over 5 workers? Assuming your code is fully parallelizable, how much faster would a run with 9 workers be compared to a run with 5 workers? Express both numbers as a percentage. How well, in your opinion, does Hadoop parallelize your code? Justify your answer in 1-2 sentences.
Theoretically 9 workers should be about 80% faster than 5 workers.
However the actually speed increase experienced was about 68% faster.
This means the experienced reality was within about 17 percent of the expected theoretical max increased speed. This means it was highly parallelizable.

5. For a single run on the 2006 dataset, what was the price per GB processed on with 5 workers? With 9 workers? (Recall that an extra-large instance costs $0.58 per hour, rounded up to the nearest hour.) Express your answers in terms of dollars/GB.
If we compare the third and forth runs for (capital, 0) our results are:
$0.82 for 5 workers.
$0.88 for 9 workers.

6. How much total money did you use to complete this project?
Considering that they round up for the hours, and both instances took over an hour to complete. It cost around $16.24.